'''
You work for an early-stage startup in Germany. Your team has been working on a redesign of the landing page. 
The team believes a new design will increase the number of people who click through and join your site. 
They have been testing the changes for a few weeks and now they want to measure the impact of the change and 
need you to determine if the increase can be due to random chance or if it is statistically significant.

The team assembled the following file:

"treatment" - "yes" if the user saw the new version of the landing page, no otherwise.
"new_images" - "yes" if the page used a new set of images, no otherwise.
"converted" - 1 if the user joined the site, 0 otherwise.
The control group is those users with "no" in both columns: the old version with the old set of images.
'''

import pandas as pd
df = pd.read_csv('./data/redesign.csv')
#print(df.head()) #  treatment new_images  converted
#print(type(df)) #<class 'pandas.core.frame.DataFrame'>
df = df.replace(to_replace = ['yes','no'],value = [1,0])
df=df.dropna()

'''
What is the correlation of the treatment and new_images individually?
Maybe one column is effective while the other is not?
The following code will find out:
'''

treatmentCorr=df['treatment'].corr(df['converted'])
new_imagesCorr=(df['new_images'].corr(df['converted']))
print(round(treatmentCorr, 4) , " is the treatment correlation.")
print(round(new_imagesCorr, 4) , " is the new images correlation.")

'''
Correlation Summary:
0.0111  is the treatment correlation.
-0.0007  is the new images correlation.

With a correlation close to 0, this shows that there seems to be no correlation between the two individual columns and becoming a customer.
'''


#LOGISTIC REGRESSION:
from sklearn.model_selection import train_test_split # splitting the data
from sklearn.linear_model import LogisticRegression # model algorithm
from sklearn import metrics
import matplotlib.pyplot as plt

#Split the data set into x and y data:
y_data = df['converted']
#x_data = df['new_images']
x_data = df.drop('converted', axis = 1)

#split the dataset into training (70%) and testing (30%) sets:
from sklearn.model_selection import train_test_split
x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x_data, y_data, test_size = 0.3, random_state=42)
#Logistic regression defaults to L2

#Create the model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

#Train the model and create predictions
model.fit(x_training_data, y_training_data)
predictions = model.predict(x_test_data)
#print(predictions)

#Generate a confusion matrix
from sklearn.metrics import confusion_matrix, roc_auc_score

#use model to predict probability that given y value is 1:
y_pred_proba = model.predict_proba(x_test_data)[::,1]

auc = round( metrics.roc_auc_score(y_test_data, y_pred_proba), 4 ) 
print("Logistic model AUC is: ", auc)  # AUC is:  0.5059

'''
LOGISTIC REGRESSION SUMMARY:
With logistic regression using both treatment and new_image columns,
the AUC was 0.5059. This shows little to no correlation between the new website and attracting new customers.
'''



'''
FINAL SUMMARY:
With logistic regression using both treatment and new_image columns,
the AUC was 0.5059. This shows no correlation between the new website.

0.0111  is the treatment correlation.
-0.0007  is the new images correlation.
AUC is:  0.5059

With a correlation close to 0, this shows that there seems to be no correlation between the two individual columns and becoming a customer.
'''


'''
RANDOM FOREST REGRESSION
'''

import pandas as pd
import numpy as np
import time

#Visalization libraries
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

''' STEP2: TRAIN THE DATA '''

#Split the data set into training data and test data
from sklearn.model_selection import train_test_split
x = df.drop('converted', axis = 1)
y = df['converted']
x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x, y, test_size = 0.3, random_state=42)

#Train the decision tree model
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(x_training_data, y_training_data)
predictions = model.predict(x_test_data)

''' STEP3: PREDICT THE DATA '''

#Measure the performance of the decision tree model
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, r2_score
print(classification_report(y_test_data, predictions))
print(confusion_matrix(y_test_data, predictions))

#Train the random forests model
from sklearn.ensemble import RandomForestClassifier
random_forest_model = RandomForestClassifier()
random_forest_model.fit(x_training_data, y_training_data)
random_forest_predictions = random_forest_model.predict(x_test_data)

'''
STEP 3: MAKE PREDICTIONS:
'''

#Make predictions with the model
predictions = model.predict(x_test_data)

#Performance measurement:
import sklearn.metrics as metrics
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
#print(classification_report(y_test_data, predictions))
#print(confusion_matrix(y_test_data, predictions))

metrics.mean_absolute_error(y_test_data, predictions)
metrics.mean_squared_error(y_test_data, predictions)
np.sqrt(metrics.mean_squared_error(y_test_data, predictions))

#print(classification_report(y_test_data, predictions))
#print(confusion_matrix(y_test_data, predictions))

#use model to predict probability that given y value is 1:
y_pred_proba = model.predict(x_test_data)

#calculate AUC of model=
auc = round( metrics.roc_auc_score(y_test_data, y_pred_proba), 4 ) 
print("Random forest AUC is: ", auc)

'''
  precision    recall  f1-score   support

           0       0.89      1.00      0.94     10792
           1       0.00      0.00      0.00      1354

    accuracy                           0.89     12146
   macro avg       0.44      0.50      0.47     12146
weighted avg       0.79      0.89      0.84     12146

[[10792     0]
 [ 1354     0]]
AUC is:  0.5
'''

'''
RANDOM FOREST FINAL SUMMARY:
The random forest AUC seems to be identical to the logistic regression AUC.
Therefore, it seems that there is little to no correlation for the new website.
'''

