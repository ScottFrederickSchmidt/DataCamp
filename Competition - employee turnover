import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

'''
Data Manipulation:
'''

df = pd.read_csv('./data/employee_churn_data.csv') # (9540, 10)
#These columns are numeric:
cols=['promoted', 'review', 'projects', 'tenure', 
'satisfaction','bonus','avg_hrs_month']

y=df['left'].replace({'no': 0, 'yes': 1}).astype(int)
df1=df.filter(cols).dropna()  #[9540 rows x 7 columns] #no missing values

#DETECTING Multicollinearity using a heatmap and VIF:
Var_Corr = df.corr()
# plot the heatmap and annotation on it
heat=sns.heatmap(Var_Corr, xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, cmap='Greens', annot=True)
#print(heat)
'''
The heatmap shows a .98 correlation between avg_hrs_month and tenure.
This was the only major Multicollinearity detected. 
Therefore, one of these variables should be removed.
'''

# Import library for VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):
    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return(vif)

calc=calc_vif(df[cols])
#print(calc)
'''
       variables         VIF
0       promoted    1.031462
1         review   71.424766
2       projects   32.971590
3         tenure   35.635391
4   satisfaction   13.329373
5          bonus    1.269142
6  avg_hrs_month  230.920778
'''


'''
Are the different correlations correlated? 
1)Normal correlation using .corr in Python. 
2)Pearson's correlation assesses linear relationships between two continuous variables.
3)Correlationn coefficient represents the linear dependence of two variables or sets of data.
'''

dCor={} #correlation dictionary
dPear={} #pearson dictionary 
dCov={} #covariance correlation dictionary
dKend={} #Kendalltau correlation dictionary

for col in cols:
    #Correlation:
    x=df[col]
    cor=y.corr(x).round(3)
    dCor[col]=cor
    
    #pearson correlation:
    pear=np.corrcoef(list(x), list(y))[0, 1].round(3)
    dPear[col]=pear
    
    #covariance correlation
    cov=np.cov(x,y)[0][1].round(3)
    dCov[col]=cov
    
    #kendall correlation:

    
dCor=sorted(((v, k) for k, v in dCor.items()), reverse=True)
dPear=sorted(((v, k) for k, v in dPear.items()), reverse=True)
dCov=sorted(((v, k) for k, v in dCov.items()), reverse=True)

print("Calculating Correlations:")
for d in dCor:
    print(d)
print("")

print("Calculate Pearson Correlation:")
for p in dPear:
    print(p)
print("")

print("Covariance correlation:")
for c in dCov:
    print(c)
    
'''
Calculating Correlations:
(0.304, 'review')
(0.011, 'tenure')
(0.009, 'avg_hrs_month')
(-0.01, 'satisfaction')
(-0.011, 'bonus')
(-0.012, 'projects')
(-0.037, 'promoted')

Calculate Pearson Correlation:
(0.304, 'review')
(0.011, 'tenure')
(0.009, 'avg_hrs_month')
(-0.01, 'satisfaction')
(-0.011, 'bonus')
(-0.012, 'projects')
(-0.037, 'promoted')

Covariance correlation:
(0.017, 'avg_hrs_month')
(0.012, 'review')
(0.007, 'tenure')
(-0.001, 'satisfaction')
(-0.002, 'bonus')
(-0.003, 'promoted')
(-0.003, 'projects')
'''
